"""
==============================================================================
çŸ¥ç§‹æ¨¡å‹ V8.5: æ•°æ®æŒä¹…åŒ–ç‰ˆ (Zhiqiu Model V8.5)

ç‰ˆæœ¬æ¼”è¿›å†å² (Version Evolution History):
-----------------------------------------
v8.4 -> v8.5 (å½“å‰ç‰ˆæœ¬ - æ•°æ®æŒä¹…åŒ–ç‰ˆ):
    - [æ ¸å¿ƒåŠŸèƒ½] æ–°å¢æ­¥éª¤12ï¼Œåœ¨ç¨‹åºè¿è¡Œç»“æŸåï¼Œä½¿ç”¨pickleå°†æ‰€æœ‰ç”¨äºåç»­
      åˆ†æå’Œç»˜å›¾çš„å˜é‡ï¼ˆåŒ…æ‹¬ç»“æœå­—å…¸ã€é¢„æµ‹æ•°ç»„ã€é…ç½®ä¿¡æ¯ç­‰ï¼‰ä¿å­˜åˆ°
      ä¸€ä¸ª`.pkl`æ–‡ä»¶ä¸­ã€‚
    - [ç§‘ç ”æµç¨‹ä¼˜åŒ–] æ­¤åŠŸèƒ½å®ç°äº†â€œè®¡ç®—â€ä¸â€œå¯è§†åŒ–â€çš„å½»åº•åˆ†ç¦»ï¼Œç”¨æˆ·
      æœªæ¥å¯ä»¥ç¼–å†™ç‹¬ç«‹çš„è½»é‡çº§è„šæœ¬æ¥åŠ è½½æ­¤`.pkl`æ–‡ä»¶ï¼Œæ— é™æ¬¡ã€
      å¿«é€Ÿåœ°é‡æ–°ç”Ÿæˆæˆ–ä¿®æ”¹å›¾è¡¨ï¼Œæå¤§æå‡äº†ç§‘ç ”æ•ˆç‡ã€‚

v8.3 -> v8.4 (ä¿®å¤ä¸å¢å¼ºç‰ˆ):
    - [ä»£ç è´¨é‡] é‡‡çº³Bugåˆ†ææŠ¥å‘Šå»ºè®®ï¼Œå¯¹ä»£ç è´¨é‡å’Œå¥å£®æ€§è¿›è¡Œå…¨é¢ä¼˜åŒ–ã€‚
    - [BUGä¿®å¤-æ¾„æ¸…] é’ˆå¯¹æŠ¥å‘Šä¸­æŒ‡å‡ºçš„â€œä¸¥é‡Bug #1, #2â€ï¼Œç»å¤æ ¸ç¡®è®¤åŸä»£ç é€»è¾‘
      æ­£ç¡®ï¼Œä¸ºé¿å…è¯¯è§£ï¼Œåœ¨ç›¸åº”ä½ç½®è¡¥å……äº†è¶…è¯¦ç»†çš„â€œä¿å§†çº§æ³¨é‡Šâ€ä»¥é˜æ˜å·¥ä½œåŸç†ã€‚
    - [BUGä¿®å¤-ä¿®æ­£] é’ˆå¯¹æŠ¥å‘Šä¸­æŒ‡å‡ºçš„â€œæ½œåœ¨Bug #3, #4â€ï¼Œè™½ç„¶åŸä»£ç é€šè¿‡min_len
      ç­‰æ–¹å¼å·²è§„é¿äº†é£é™©ï¼Œä½†ä¸ºå¢å¼ºé€»è¾‘æ¸…æ™°åº¦ï¼Œè¡¥å……äº†æ›´æ˜ç¡®çš„é•¿åº¦æ ¡éªŒå’Œæ³¨é‡Šã€‚
    - [ä»£ç è´¨é‡] ç§»é™¤äº†æœªä½¿ç”¨çš„`import time`ã€‚
    - [ä»£ç è´¨é‡] ä¸ºå…³é”®çš„æ–‡ä»¶åŠ è½½æ­¥éª¤å¢åŠ äº†`try-except`å¼‚å¸¸å¤„ç†ï¼Œå¢å¼ºç¨‹åºå¥å£®æ€§ã€‚
    - [æ€§èƒ½ä¼˜åŒ–] åœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ç»“æŸåï¼Œå¢åŠ äº†`torch.cuda.empty_cache()`ï¼Œ
      è¾…åŠ©GPUè¿›è¡Œæ˜¾å­˜ç®¡ç†ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ã€‚
    - [å¯è¯»æ€§] å¯¹éƒ¨åˆ†è¿‡é•¿çš„å•è¡Œä»£ç è¿›è¡Œäº†æ¢è¡Œå¤„ç†ã€‚
==============================================================================
"""

import torch
import torch.nn as nn
import numpy as np
import xarray as xr
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from tqdm import tqdm
import datetime
import pandas as pd
import random
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import math
import os
import warnings
import json
import pickle
import gc  # å†…å­˜ç®¡ç†
warnings.filterwarnings("ignore", category=UserWarning)

# ==============================================================================
# 0. å‚æ•°æ€»æ§ (Master Configuration) - V8.5
# ==============================================================================
print("--- æ­¥éª¤ 0: å‚æ•°è®¾ç½® (V8.5 æ•°æ®æŒä¹…åŒ–ç‰ˆ) ---")

# éšæœºç§å­è®¾ç½®
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# æ–‡ä»¶è·¯å¾„è®¾ç½®
ERA5_T2M_PATH = 'ERA5_2010_2023_sp_t2m.nc'
ERA5_SST_MSL_PATH = 'ERA5_2010_2023_sst_msl_d2m.nc'
ERA5_SSR_PATH = 'ERA5_2010_2023_ssr_avg_snswrf_snlwrf.nc'
SIC_PATH = './arco/processed_sic_monthly.nc'
DRIFT_PATH = './weekly/processed_drift_monthly.nc'
MODEL_SAVE_PATH = './models/'
RESULTS_SAVE_PATH = './results/'

# å®éªŒå‚æ•°
DATA_SUBSET = None
FEATURE_ENGINEERING_VERSION = 'B'
MODEL_VERSION = '3'  # å¯é€‰: 'lstm', '1', '2', '3'
HORIZON = 1
N_MC_SAMPLES = 50

# è®­ç»ƒè¶…å‚æ•°
SEQUENCE_LENGTH = 6
BATCH_SIZE = 256
NUM_EPOCHS = 30
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-5
EARLY_STOP_PATIENCE = 10 # å»ºè®®é€‚å½“å¢åŠ ä»¥è¿›è¡Œæ›´å……åˆ†çš„è®­ç»ƒ
EFFECTIVE_NUM_WORKERS = 0 if os.name == "nt" else 2

# è®­ç»ƒç¨³å®šæ€§å‚æ•°
GRADIENT_CLIP_VALUE = 1.0
NAN_CHECK_FREQUENCY = 100
LOSS_EXPLOSION_THRESHOLD = 100.0

# è¿è¡Œæ ‡è¯†
RUN_TAG = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
print(f"æœ¬æ¬¡è¿è¡Œå”¯ä¸€æ ‡è¯† (Run Tag): {RUN_TAG}")
os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
os.makedirs(RESULTS_SAVE_PATH, exist_ok=True)

# ==============================================================================
# I. å¥å£®æ€§éªŒè¯å’Œå·¥ç¨‹å®è·µå·¥å…·
# ==============================================================================
print("\n--- I: åˆå§‹åŒ–å¥å£®æ€§éªŒè¯å·¥å…· ---")

def check_required_files():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚"""
    required_files = [ERA5_T2M_PATH, ERA5_SST_MSL_PATH, ERA5_SSR_PATH, SIC_PATH, DRIFT_PATH]
    for file_path in required_files:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"å…³é”®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        print(f"âœ… æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {file_path}")
    print("âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ£€æŸ¥é€šè¿‡ã€‚")

def validate_data_alignment(*datasets):
    """éªŒè¯æ‰€æœ‰æ•°æ®é›†çš„æ—¶ç©ºå¯¹é½ã€‚"""
    if len(datasets) < 2: return True
    print("ğŸ” å¼€å§‹éªŒè¯æ•°æ®æ—¶ç©ºå¯¹é½...")
    reference = datasets[0]
    for i, ds in enumerate(datasets[1:], 1):
        if not np.array_equal(ds.coords['time'].values, reference.coords['time'].values):
            raise ValueError(f"æ•°æ®é›† {i} æ—¶é—´åæ ‡ä¸å¯¹é½")
        for coord in ['y', 'x']:
            if coord in ds.coords and coord in reference.coords:
                if not np.allclose(ds.coords[coord], reference.coords[coord], atol=1e-6):
                    raise ValueError(f"æ•°æ®é›† {i} çš„ {coord} åæ ‡ä¸å¯¹é½")
    print("âœ… æ‰€æœ‰æ•°æ®é›†æ—¶ç©ºå¯¹é½éªŒè¯é€šè¿‡ã€‚")

def validate_data_quality(data, var_name):
    """åŸºäºå˜é‡ç±»å‹çš„æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯ä¸NaNå¡«å……ã€‚"""
    print(f"ğŸ” å¼€å§‹éªŒè¯ {var_name} æ•°æ®è´¨é‡...")
    nan_count = np.isnan(data.values).sum()
    if nan_count > 0:
        print(f"âš ï¸  {var_name} åŒ…å« {nan_count:,} ä¸ªNaNå€¼")
        var_lower = var_name.lower()
        if 'sic' in var_lower or 'seaice' in var_lower:
            fill_value = 0.0
            print(f"   ğŸ§Š æµ·å†°æµ“åº¦å˜é‡ç”¨ {fill_value} å¡«å……")
            data = data.fillna(fill_value)
        elif any(temp_var in var_lower for temp_var in ['t2m', 'sst']):
            fill_value = float(data.mean().values)
            print(f"   ğŸŒ¡ï¸  æ¸©åº¦å˜é‡ç”¨å‡å€¼ {fill_value:.2f} å¡«å……")
            data = data.fillna(fill_value)
        else:
            fill_value = float(data.median().values)
            print(f"   ğŸ“Š å…¶ä»–å˜é‡ç”¨ä¸­ä½æ•° {fill_value:.4f} å¡«å……")
            data = data.fillna(fill_value)
    if np.isinf(data.values).sum() > 0:
        raise ValueError(f"{var_name} åŒ…å«æ— é™å€¼")
    print(f"âœ… {var_name} æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡ã€‚")
    return data

def optimize_memory_usage():
    gc.collect()
    if torch.cuda.is_available(): torch.cuda.empty_cache()

check_required_files()

# ==============================================================================
# II. æ¨¡å‹å®šä¹‰åº“ (Model Definition Library) - V8.1 å®Œæ•´ç‰ˆ
# ==============================================================================
print("\n--- II: å®šä¹‰æ¨¡å‹åº“ (V8.1 å®Œæ•´ç‰ˆ) ---")

def _assert_divisible(d, n):
    """ç¡®ä¿dèƒ½è¢«næ•´é™¤ã€‚"""
    if d % n != 0: 
        raise ValueError(f"d_model ({d}) must be divisible by num_heads ({n}).")

def _build_pos_encoding(max_len: int, d_model: int):
    """æ„å»ºä½ç½®ç¼–ç ã€‚"""
    pe = torch.zeros(max_len, d_model, dtype=torch.float32)
    position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe.unsqueeze(0)

def _causal_mask(L: int, device: torch.device):
    """ç”Ÿæˆå› æœæ©ç ã€‚"""
    return torch.triu(torch.ones(L, L, device=device, dtype=torch.bool), diagonal=1)

class LSTMModel(nn.Module):
    """
    LSTMåŸºç¡€æ¨¡å‹ï¼šä½œä¸ºå¾ªç¯ç¥ç»ç½‘ç»œ (RNN) çš„ä»£è¡¨ï¼Œæ˜¯Transformerå‡ºç°å‰çš„ä¸»æµåºåˆ—æ¨¡å‹ã€‚
    """
    def __init__(self, i_dim, h_dim, n_layers, o_dim, drop=0.2):
        super().__init__()
        self.lstm = nn.LSTM(i_dim, h_dim, n_layers, batch_first=True, dropout=drop if n_layers>1 else 0)
        self.fc = nn.Sequential(
            nn.Linear(h_dim, h_dim // 2), 
            nn.ReLU(), 
            nn.Dropout(drop), 
            nn.Linear(h_dim // 2, o_dim), 
            nn.Sigmoid()
        )
    
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

class ImprovedSequenceTransformer_v1(nn.Module):
    """
    Transformer v1: åŸºç¡€Transformer Encoderæ¶æ„ã€‚
    """
    def __init__(self, i_dim, m_dim, n_head, n_layers, o_dim, drop=0.1, max_len=5000):
        super().__init__()
        _assert_divisible(m_dim, n_head)
        self.input_fc = nn.Linear(i_dim, m_dim)
        self.register_buffer("pos_encoding", _build_pos_encoding(max_len, m_dim), persistent=False)
        enc_layer = nn.TransformerEncoderLayer(m_dim, n_head, batch_first=True, dropout=drop, activation="gelu")
        self.encoder = nn.TransformerEncoder(enc_layer, n_layers)
        self.output_fc = nn.Sequential(
            nn.Linear(m_dim, m_dim // 2),
            nn.ReLU(),
            nn.Dropout(drop),
            nn.Linear(m_dim // 2, o_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        _, L, _ = x.shape
        x = self.input_fc(x)
        x = x + self.pos_encoding[:, :L, :].to(x.device, dtype=x.dtype)
        x = self.encoder(x, mask=_causal_mask(L, x.device))
        return self.output_fc(x[:, -1, :])

class ImprovedSequenceTransformer_v2(nn.Module):
    """
    Transformer v2: åœ¨v1çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†åºåˆ—å¹³å‡çš„æ··åˆç­–ç•¥ã€‚
    """
    def __init__(self, i_dim, m_dim, n_head, n_layers, o_dim, drop=0.1, max_len=5000, blend=0.1):
        super().__init__()
        _assert_divisible(m_dim, n_head)
        self.blend = blend
        self.input_fc = nn.Linear(i_dim, m_dim)
        self.register_buffer("pos_encoding", _build_pos_encoding(max_len, m_dim), persistent=False)
        enc_layer = nn.TransformerEncoderLayer(m_dim, n_head, batch_first=True, dropout=drop, activation="gelu")
        self.encoder = nn.TransformerEncoder(enc_layer, n_layers)
        self.output_fc = nn.Sequential(
            nn.Linear(m_dim, m_dim // 2),
            nn.ReLU(),
            nn.Dropout(drop),
            nn.Linear(m_dim // 2, o_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        _, L, _ = x.shape
        x = self.input_fc(x)
        x = x + self.pos_encoding[:, :L, :].to(x.device, dtype=x.dtype)
        x = self.encoder(x, mask=_causal_mask(L, x.device))
        x = x[:, -1, :] + self.blend * x.mean(dim=1)
        return self.output_fc(x)

class ImprovedSequenceTransformer_v3(nn.Module):
    """
    Transformer v3: å¼•å…¥äº†CLS tokençš„å…ˆè¿›æ¶æ„ï¼Œå€Ÿé‰´è‡ªBERT/ViTã€‚
    """
    def __init__(self, i_dim, m_dim, n_head, n_layers, o_dim, drop=0.1, max_len=5000):
        super().__init__()
        _assert_divisible(m_dim, n_head)
        self.input_fc = nn.Sequential(nn.Linear(i_dim, m_dim), nn.LayerNorm(m_dim))
        self.cls_token = nn.Parameter(torch.randn(1, 1, m_dim) * 0.02)
        self.register_buffer("pos_encoding", _build_pos_encoding(max_len + 1, m_dim), persistent=False)
        enc_layer = nn.TransformerEncoderLayer(m_dim, n_head, batch_first=True, dropout=drop, activation="gelu")
        self.encoder = nn.TransformerEncoder(enc_layer, n_layers)
        self.output_fc = nn.Sequential(
            nn.Linear(m_dim, m_dim // 2), nn.GELU(), nn.Dropout(drop),
            nn.Linear(m_dim // 2, m_dim // 4), nn.GELU(), nn.Dropout(drop),
            nn.Linear(m_dim // 4, o_dim), nn.Sigmoid()
        )
    
    def forward(self, x):
        B, L, _ = x.shape
        x = self.input_fc(x)
        cls = self.cls_token.repeat(B, 1, 1)
        x = torch.cat([cls, x], dim=1)
        x = x + self.pos_encoding[:, :(L+1), :].to(x.device, dtype=x.dtype)
        mask = _causal_mask(L + 1, x.device)
        mask[0, :] = False
        x = self.encoder(x, mask=mask)
        return self.output_fc(x[:, 0, :])

def get_model(version, X_train_shape):
    """
    æ ¹æ®ç‰ˆæœ¬å·è·å–å¯¹åº”çš„æ¨¡å‹å®ä¾‹ (å®Œæ•´ç‰ˆ)ã€‚
    """
    i_dim = X_train_shape[2]
    v_str = str(version).lower()
    
    if v_str == 'lstm':
        print("ğŸ“¦ æ­£åœ¨å®ä¾‹åŒ–æ¨¡å‹: LSTM (åŸºçº¿)")
        return LSTMModel(i_dim, 128, 2, 1)
    elif v_str == '1':
        print("ğŸ“¦ æ­£åœ¨å®ä¾‹åŒ–æ¨¡å‹: Transformer v1")
        return ImprovedSequenceTransformer_v1(i_dim, 128, 4, 2, 1)
    elif v_str == '2':
        print("ğŸ“¦ æ­£åœ¨å®ä¾‹åŒ–æ¨¡å‹: Transformer v2")
        return ImprovedSequenceTransformer_v2(i_dim, 128, 4, 3, 1)
    elif v_str == '3':
        print("ğŸ“¦ æ­£åœ¨å®ä¾‹åŒ–æ¨¡å‹: Transformer v3 (CLS Token)")
        return ImprovedSequenceTransformer_v3(i_dim, 256, 8, 4, 1)
    else:
        raise ValueError("MODEL_VERSION å¿…é¡»æ˜¯ 'lstm', '1', '2', æˆ– '3'")

# ==============================================================================
# III. å¢å¼ºéªŒè¯å’Œä¿®å¤å·¥å…·
# ==============================================================================
print("\n--- III: åˆå§‹åŒ–å¢å¼ºéªŒè¯å·¥å…· ---")
class StrictMCDropout:
    def __init__(self, model, dropout_layers_only=True): self.model = model; self.dropout_layers_only = dropout_layers_only; self.original_training_states = {}
    def __enter__(self):
        for name, module in self.model.named_modules(): self.original_training_states[name] = module.training
        if self.dropout_layers_only: self.model.eval(); [m.train() for m in self.model.modules() if isinstance(m, nn.Dropout)]
        else: self.model.train()
        return self
    def __exit__(self, exc_type, exc_val, exc_tb):
        for name, module in self.model.named_modules(): module.train() if self.original_training_states[name] else module.eval()
def compute_sie_with_validation(sic, area_map, thresh=0.15):
    if sic.shape != area_map.shape: raise ValueError(f"SICç»´åº¦ {sic.shape} ä¸é¢ç§¯å›¾ç»´åº¦ {area_map.shape} ä¸åŒ¹é…")
    mask = (sic >= thresh).astype(np.float32); return float((mask * area_map).sum())
def comprehensive_data_validation(X_train_seq, X_val_seq, X_test_seq, y_train_seq, y_val_seq, y_test_seq, num_pixels, SEQUENCE_LENGTH, HORIZON, X_train_raw, X_val_raw, X_test_raw):
    print("ğŸ” æ‰§è¡Œç»¼åˆæ•°æ®å®Œæ•´æ€§éªŒè¯...")
    L_total = SEQUENCE_LENGTH + (HORIZON - 1)
    def nseq(T, L): return max(0, T - L)
    train_T, val_T, test_T = X_train_raw.shape[0], X_val_raw.shape[0], X_test_raw.shape[0]
    expected_samples = (nseq(train_T, L_total) + nseq(val_T, L_total) + nseq(test_T, L_total)) * num_pixels
    total_samples = len(X_train_seq) + len(X_val_seq) + len(X_test_seq)
    if total_samples != expected_samples: print(f"âŒ æ ·æœ¬æ•°é‡ä¸ä¸€è‡´! æœŸæœ›: {expected_samples:,}, å®é™…: {total_samples:,}")
    else: print("âœ… æ ·æœ¬æ•°é‡ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡ã€‚")

# ==============================================================================
# IV. ä¸»æµç¨‹
# ==============================================================================

# ------------------------------------------------------------------------------
# æ­¥éª¤ 1: åŠ è½½å·²å¤„ç†å¥½çš„æ•°æ®
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 1: åŠ è½½å·²å¤„ç†å¥½çš„æ•°æ® ---")
try:
    sic_ds = xr.open_dataset(SIC_PATH)
    drift_ds = xr.open_dataset(DRIFT_PATH)
    t2m_ds = xr.open_dataset(ERA5_T2M_PATH)
    sst_msl_ds = xr.open_dataset(ERA5_SST_MSL_PATH)
    ssr_ds = xr.open_dataset(ERA5_SSR_PATH)
except FileNotFoundError as e:
    print(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    raise

datasets_to_process = [sic_ds, drift_ds, t2m_ds, sst_msl_ds, ssr_ds]
processed_datasets = []
for ds in datasets_to_process:
    if 'valid_time' in ds.coords: ds = ds.rename({'valid_time': 'time'})
    if 'time' in ds.coords and ds['time'].dtype != 'datetime64[ns]':
        try:
            ds['time'] = ds.indexes['time'].to_datetimeindex()
        except:
            ds['time'] = pd.to_datetime([datetime.datetime(t.year, t.month, t.day) for t in ds['time'].values])
    processed_datasets.append(ds)

sic_ds, drift_ds, t2m_ds, sst_msl_ds, ssr_ds = processed_datasets
sic_monthly = sic_ds['cdr_seaice_conc_monthly']
drift_u_monthly = drift_ds['u']
drift_v_monthly = drift_ds['v']
t2m_monthly = t2m_ds['t2m']
sst_monthly = sst_msl_ds['sst']
msl_monthly = sst_msl_ds['msl']
ssr_monthly = ssr_ds['ssr']

time_coords = sic_monthly['time'].values
start_date_str = pd.to_datetime(time_coords[0]).strftime('%Y-%m-%d')
end_date_str = pd.to_datetime(time_coords[-1]).strftime('%Y-%m-%d')
print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°æ•°æ®æ—¶é—´èŒƒå›´: {start_date_str} åˆ° {end_date_str}")

all_unaligned = [sic_monthly, drift_u_monthly, drift_v_monthly, t2m_monthly, sst_monthly, msl_monthly, ssr_monthly]
all_aligned = [sic_monthly]
for ds in all_unaligned[1:]:
    all_aligned.append(ds.reindex_like(sic_monthly, method="nearest"))
validate_data_alignment(*all_aligned)

dataset_names = ['SIC', 'Drift_U', 'Drift_V', 'T2M', 'SST', 'MSL', 'SSR']
unpacked_datasets = [validate_data_quality(ds, name) for ds, name in zip(all_aligned, dataset_names)]
sic_monthly, drift_u_monthly, drift_v_monthly, t2m_monthly, sst_monthly, msl_monthly, ssr_monthly = unpacked_datasets

grid_shape = sic_monthly.shape[1:]
num_timesteps = len(time_coords)
num_pixels = grid_shape[0] * grid_shape[1]
CELL_AREA_KM2_MAP = np.full(grid_shape, 25.0 * 25.0, dtype=np.float32)
print(f"ğŸ“‹ æ•°æ®é›†åŸºæœ¬ä¿¡æ¯: ç½‘æ ¼å½¢çŠ¶={grid_shape}, æ—¶é—´æ­¥æ•°={num_timesteps}, åƒç´ ç‚¹æ•°={num_pixels:,}")

# ------------------------------------------------------------------------------
# æ­¥éª¤ 2: æ„å»ºç‰¹å¾çŸ©é˜µ
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 2: æ„å»ºç‰¹å¾çŸ©é˜µ ---")
flat_sic = sic_monthly.values.reshape(num_timesteps, -1)
flat_drift_u = drift_u_monthly.values.reshape(num_timesteps, -1)
flat_drift_v = drift_v_monthly.values.reshape(num_timesteps, -1)
flat_t2m = t2m_monthly.values.reshape(num_timesteps, -1)
flat_sst = sst_monthly.values.reshape(num_timesteps, -1)
flat_ssr = ssr_monthly.values.reshape(num_timesteps, -1)
flat_msl = msl_monthly.values.reshape(num_timesteps, -1)

months = pd.to_datetime(time_coords).month.to_numpy()
month_sin = np.sin(2 * np.pi * (months - 1) / 12).reshape(-1, 1).repeat(num_pixels, axis=1)
month_cos = np.cos(2 * np.pi * (months - 1) / 12).reshape(-1, 1).repeat(num_pixels, axis=1)
features_matrix = np.stack([month_sin[:-1], month_cos[:-1], flat_sic[:-1], flat_drift_u[:-1], flat_drift_v[:-1], flat_t2m[:-1], flat_sst[:-1], flat_ssr[:-1], flat_msl[:-1]], axis=2)
all_data_X = features_matrix
all_data_y = flat_sic[1:]
feature_names = ['month_sin', 'month_cos', 'sic', 'drift_u', 'drift_v', 't2m', 'sst', 'ssr', 'msl']

# ------------------------------------------------------------------------------
# æ­¥éª¤ 3: æ—¶åºåˆ‡åˆ†ä¸æ ‡å‡†åŒ–
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 3: æ—¶åºåˆ‡åˆ†ä¸æ ‡å‡†åŒ– ---")
y_time_coords = time_coords[1:]
train_end_idx = int(len(all_data_X) * 0.7)
val_end_idx = int(len(all_data_X) * 0.9)
X_train_raw, y_train_raw = all_data_X[:train_end_idx], all_data_y[:train_end_idx]
X_val_raw, y_val_raw = all_data_X[train_end_idx:val_end_idx], all_data_y[train_end_idx:val_end_idx]
X_test_raw, y_test_raw = all_data_X[val_end_idx:], all_data_y[val_end_idx:]
train_y_times = y_time_coords[:train_end_idx]

print("ğŸ”§ æ‰§è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
X_train_flat = X_train_raw.reshape(-1, X_train_raw.shape[2])
scaler_X = StandardScaler().fit(X_train_flat)
scaler_path = os.path.join(MODEL_SAVE_PATH, f"scaler_{RUN_TAG}.pkl")
with open(scaler_path, 'wb') as f: pickle.dump(scaler_X, f)
print(f"ğŸ’¾ æ ‡å‡†åŒ–å™¨ (Scaler) å·²ä¿å­˜åˆ°: {scaler_path}")
def scale_X(X_raw, sX): return sX.transform(X_raw.reshape(-1, X_raw.shape[2])).reshape(X_raw.shape)
X_train_s, y_train_s = scale_X(X_train_raw, scaler_X), y_train_raw
X_val_s, y_val_s = scale_X(X_val_raw, scaler_X), y_val_raw
X_test_s, y_test_s = scale_X(X_test_raw, scaler_X), y_test_raw

# ------------------------------------------------------------------------------
# æ­¥éª¤ 4: åˆ›å»ºæ—¶åºæ ·æœ¬ä¸æ•°æ®åŠ è½½å™¨
# ------------------------------------------------------------------------------
print(f"\n--- æ­¥éª¤ 4: åˆ›å»ºæ—¶åºæ ·æœ¬ ---")
def create_sequences(X, y, seq_len, horizon):
    Xs, ys = [], []
    for i in range(len(X) - seq_len - (horizon - 1)):
        Xs.append(X[i : i + seq_len])
        ys.append(y[i + seq_len + (horizon - 1)])
    Xs, ys = np.array(Xs), np.array(ys)
    return torch.FloatTensor(Xs.transpose(0, 2, 1, 3).reshape(-1, seq_len, X.shape[2])), torch.FloatTensor(ys.reshape(-1, 1))

X_train_seq, y_train_seq = create_sequences(X_train_s, y_train_s, SEQUENCE_LENGTH, HORIZON)
X_val_seq, y_val_seq = create_sequences(X_val_s, y_val_s, SEQUENCE_LENGTH, HORIZON)
X_test_seq, y_test_seq = create_sequences(X_test_s, y_test_s, SEQUENCE_LENGTH, HORIZON)
comprehensive_data_validation(X_train_seq, X_val_seq, X_test_seq, y_train_seq, y_val_seq, y_test_seq, num_pixels, SEQUENCE_LENGTH, HORIZON, X_train_raw, X_val_raw, X_test_raw)

class SequenceDataset(Dataset):
    def __init__(self, x, y): self.x, self.y = x, y
    def __len__(self): return len(self.x)
    def __getitem__(self, idx): return self.x[idx], self.y[idx]

train_loader = DataLoader(SequenceDataset(X_train_seq, y_train_seq), batch_size=BATCH_SIZE, shuffle=True, num_workers=EFFECTIVE_NUM_WORKERS)
val_loader = DataLoader(SequenceDataset(X_val_seq, y_val_seq), batch_size=BATCH_SIZE, shuffle=False, num_workers=EFFECTIVE_NUM_WORKERS)

# ------------------------------------------------------------------------------
# æ­¥éª¤ 5: æ¨¡å‹å®ä¾‹åŒ–
# ------------------------------------------------------------------------------
print(f"\n--- æ­¥éª¤ 5: å®ä¾‹åŒ–æ¨¡å‹ ---")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = get_model(MODEL_VERSION, X_train_seq.shape).to(device)
criterion = nn.MSELoss()
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}, æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

# ------------------------------------------------------------------------------
# æ­¥éª¤ 6: æ¨¡å‹è®­ç»ƒ
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 6: å¼€å§‹è®­ç»ƒ ---")
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_path = os.path.join(MODEL_SAVE_PATH, f"best_model_{RUN_TAG}.pt")
training_history = {'train_loss': [], 'val_loss': []}

for epoch in range(NUM_EPOCHS):
    model.train()
    train_loss = 0
    for batch_X, batch_y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [è®­ç»ƒ]"):
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        optimizer.zero_grad()
        pred = model(batch_X)
        loss = criterion(pred, batch_y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VALUE)
        optimizer.step()
        train_loss += loss.item()
    
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch_X, batch_y in tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [éªŒè¯]"):
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            pred = model(batch_X)
            val_loss += criterion(pred, batch_y).item()
            
    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)
    training_history['train_loss'].append(avg_train_loss)
    training_history['val_loss'].append(avg_val_loss)
    
    print(f"Epoch {epoch+1:2d}: è®­ç»ƒæŸå¤±={avg_train_loss:.5f}, éªŒè¯æŸå¤±={avg_val_loss:.5f}")
    
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"    âœ… éªŒè¯æŸå¤±é™ä½ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹")
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= EARLY_STOP_PATIENCE:
            print(f"    ğŸ›‘ è§¦å‘æ—©åœ")
            break
    
    optimize_memory_usage()

# ------------------------------------------------------------------------------
# æ­¥éª¤ 7: æ¨¡å‹è¯„ä¼°
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 7: æµ‹è¯•é›†è¯„ä¼° ---")
# V8.5: æ¨èä½¿ç”¨ weights_only=True ä»¥éµå¾ªPyTorchçš„æœ€ä½³å®è·µï¼Œå¹¶æ¶ˆé™¤è­¦å‘Š
model.load_state_dict(torch.load(best_model_path, map_location=device, weights_only=True))
model.eval()
y_true = y_test_seq.numpy()
test_loader = DataLoader(SequenceDataset(X_test_seq, y_test_seq), batch_size=BATCH_SIZE*2, shuffle=False)
y_pred_list = []
mc_predictions = []

with torch.no_grad():
    for batch_X, _ in tqdm(test_loader, desc="ç¡®å®šæ€§é¢„æµ‹"):
        y_pred_list.append(model(batch_X.to(device)).cpu().numpy())
    y_pred = np.concatenate(y_pred_list, axis=0)
    
    for _ in tqdm(range(N_MC_SAMPLES), desc="MC Dropouté‡‡æ ·"):
        sample_preds = []
        with StrictMCDropout(model):
            for batch_X, _ in test_loader:
                sample_preds.append(model(batch_X.to(device)).cpu().numpy())
        mc_predictions.append(np.concatenate(sample_preds, axis=0))

mc_predictions = np.array(mc_predictions)
y_pred_mean = mc_predictions.mean(axis=0)
y_pred_std = mc_predictions.std(axis=0)

results = {
    'model_version': str(MODEL_VERSION), 
    'run_tag': RUN_TAG, 
    'best_val_loss': best_val_loss, 
    'epochs_trained': epoch + 1
}
results['rmse'] = float(np.sqrt(mean_squared_error(y_true, y_pred_mean)))
results['mae'] = float(mean_absolute_error(y_true, y_pred_mean))
results['r2'] = float(r2_score(y_true, y_pred_mean))

# ------------------------------------------------------------------------------
# æ­¥éª¤ 8 & 9: åŸºå‡†å¯¹æ¯”ä¸SIEåˆ†æ
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 8 & 9: åŸºå‡†å¯¹æ¯”ä¸SIEåˆ†æ ---")
y_test_flat = y_test_raw.ravel()
test_months = pd.to_datetime(y_time_coords[val_end_idx:]).month.to_numpy()
train_months = pd.to_datetime(train_y_times).month.to_numpy()
climo_map_monthly = np.array([y_train_raw[train_months == m].mean(axis=0) for m in range(1, 13)])
climo_pred_flat = np.array([climo_map_monthly[m-1] for m in test_months[:len(y_test_raw)]]).ravel()
persistence_pred_flat = X_test_raw[:, :, feature_names.index('sic')].ravel()

# V8.4 å…³é”®ä¿®å¤: å¼ºåˆ¶ç¡®ä¿æ‰€æœ‰ç”¨äºæ¯”è¾ƒçš„æ•°ç»„é•¿åº¦ä¸€è‡´ï¼Œé˜²æ­¢å´©æºƒ
min_len = min(len(y_test_flat), len(climo_pred_flat), len(persistence_pred_flat))
y_test_flat = y_test_flat[:min_len]
climo_pred_flat = climo_pred_flat[:min_len]
persistence_pred_flat = persistence_pred_flat[:min_len]
results['climo_rmse'] = float(np.sqrt(mean_squared_error(y_test_flat, climo_pred_flat)))
results['persistence_rmse'] = float(np.sqrt(mean_squared_error(y_test_flat, persistence_pred_flat)))

num_test_months = len(y_true) // num_pixels
test_time_coords_for_sie = y_time_coords[val_end_idx : val_end_idx + num_test_months]

sie_true, sie_pred_mean = [], []
for i in tqdm(range(num_test_months), desc="è®¡ç®—SIE"):
    true_map = y_true[i*num_pixels:(i+1)*num_pixels].reshape(grid_shape)
    pred_map = y_pred_mean[i*num_pixels:(i+1)*num_pixels].reshape(grid_shape)
    sie_true.append(compute_sie_with_validation(true_map, CELL_AREA_KM2_MAP))
    sie_pred_mean.append(compute_sie_with_validation(pred_map, CELL_AREA_KM2_MAP))

sie_true_m = np.array(sie_true) / 1e6
sie_pred_mean_m = np.array(sie_pred_mean) / 1e6
results['sie_rmse'] = float(np.sqrt(mean_squared_error(sie_true_m, sie_pred_mean_m)))
results['sie_mae'] = float(mean_absolute_error(sie_true_m, sie_pred_mean_m))

# ------------------------------------------------------------------------------
# æ­¥éª¤ 10: æœ€ç»ˆæŠ¥å‘Š
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 10: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ---")
def generate_comprehensive_report(results):
    print("\n" + "="*80); print(f"ğŸ§Š çŸ¥ç§‹æ¨¡å‹ V8.5 - æœ€ç»ˆå®éªŒæŠ¥å‘Š"); print("="*80)
    print(f"ğŸ“‹ å®éªŒé…ç½®: æ¨¡å‹ç‰ˆæœ¬='{results['model_version']}', è¿è¡Œæ ‡è¯†={results['run_tag']}")
    print(f"   è®­ç»ƒç»“æœ: å…±è®­ç»ƒ {results['epochs_trained']} è½®, æœ€ä½³éªŒè¯æŸå¤±={results['best_val_loss']:.5f}")
    print(f"\nğŸ“Š æ ¸å¿ƒç›®æ ‡: æµ·å†°å¯†é›†åº¦ (SIC) åƒç´ çº§é¢„æµ‹æ€§èƒ½:")
    print(f"   æ¨¡å‹ RMSE = {results['rmse']:.4f} (è¶Šä½è¶Šå¥½)")
    print(f"   æ°”å€™æ€åŸºå‡† RMSE = {results['climo_rmse']:.4f} (åŸºå‡†)")
    print(f"   æŒç»­æ€§åŸºå‡† RMSE = {results['persistence_rmse']:.4f} (åŸºå‡†)")
    print(f"   æ¨¡å‹ MAE  = {results['mae']:.4f}"); print(f"   æ¨¡å‹ RÂ²   = {results['r2']:.4f} (è¶Šé«˜è¶Šå¥½)")
    print(f"\nğŸ§Š å®è§‚æŒ‡æ ‡: æµ·å†°èŒƒå›´ (SIE) é¢„æµ‹æ€§èƒ½:")
    print(f"   SIE RMSE = {results['sie_rmse']:.3f} ç™¾ä¸‡kmÂ²"); print(f"   SIE MAE  = {results['sie_mae']:.3f} ç™¾ä¸‡kmÂ²"); print("="*80)
generate_comprehensive_report(results)

# ------------------------------------------------------------------------------
# æ­¥éª¤ 11 - è®ºæ–‡çº§å›¾è¡¨ç”Ÿæˆ
# ------------------------------------------------------------------------------
print("\n--- æ­¥éª¤ 11: ç”Ÿæˆè®ºæ–‡çº§å›¾è¡¨ ---")
def generate_publication_plots(results, training_history, y_true, y_pred_mean, y_pred_std, sie_true, sie_pred_mean, test_time_coords_for_sie, grid_shape, run_tag):
    print("ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾è¡¨...")
    # V8.5: å¢åŠ ä¸­æ–‡å­—ä½“è®¾ç½®ï¼Œä¿®å¤å›¾è¡¨ä¹±ç é—®é¢˜
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    plt.style.use('seaborn-v0_8-paper')
    
    # 1. SIE æ—¶é—´åºåˆ—å¯¹æ¯”å›¾
    print("   -> æ­£åœ¨ç”Ÿæˆ SIE æ—¶é—´åºåˆ—å›¾...")
    fig1, ax1 = plt.subplots(figsize=(12, 6))
    ax1.plot(test_time_coords_for_sie, sie_true, 'k-', marker='o', markersize=4, label='è§‚æµ‹å€¼ (Observation)')
    ax1.plot(test_time_coords_for_sie, sie_pred_mean, 'r-', marker='x', markersize=4, label='æ¨¡å‹é¢„æµ‹ (Forecast)')
    
    pixel_variances = y_pred_std**2
    pixel_variances_monthly = pixel_variances.reshape(num_test_months, num_pixels)
    cell_area_km2 = CELL_AREA_KM2_MAP[0, 0]
    sie_variance_km4 = pixel_variances_monthly.sum(axis=1) * (cell_area_km2**2)
    sie_std_dev_m = np.sqrt(sie_variance_km4) / 1e6
    ax1.fill_between(test_time_coords_for_sie, 
                     sie_pred_mean - 1.96 * sie_std_dev_m, 
                     sie_pred_mean + 1.96 * sie_std_dev_m, 
                     color='red', alpha=0.2, label='95% ç½®ä¿¡åŒºé—´ (95% C.I.)')
    
    ax1.set_title('æœˆåº¦åŒ—ææµ·å†°èŒƒå›´ (SIE) é¢„æµ‹ä¸è§‚æµ‹å¯¹æ¯”', fontsize=16)
    ax1.set_xlabel('æ—¥æœŸ (Date)', fontsize=12); ax1.set_ylabel('æµ·å†°èŒƒå›´ (SIE) [ç™¾ä¸‡ kmÂ²]', fontsize=12)
    ax1.legend(); ax1.grid(True, linestyle='--', alpha=0.6); fig1.tight_layout()
    fig1.savefig(os.path.join(RESULTS_SAVE_PATH, f"SIE_timeseries_{run_tag}.png"), dpi=300)
    print("      SIE å›¾å·²ä¿å­˜ã€‚")

    # 2. è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
    print("   -> æ­£åœ¨ç”ŸæˆæŸå¤±æ›²çº¿å›¾...")
    fig2, ax2 = plt.subplots(figsize=(8, 5))
    epochs = range(1, len(training_history['train_loss']) + 1)
    ax2.plot(epochs, training_history['train_loss'], 'b-o', label='è®­ç»ƒæŸå¤± (Training Loss)')
    ax2.plot(epochs, training_history['val_loss'], 'r-o', label='éªŒè¯æŸå¤± (Validation Loss)')
    best_epoch = np.argmin(training_history['val_loss']) + 1
    ax2.axvline(best_epoch, color='grey', linestyle='--', label=f'æœ€ä½³è½®æ¬¡ (Best Epoch): {best_epoch}')
    ax2.set_title('æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–', fontsize=16)
    ax2.set_xlabel('è½®æ¬¡ (Epoch)', fontsize=12); ax2.set_ylabel('å‡æ–¹è¯¯å·®æŸå¤± (MSE Loss)', fontsize=12)
    ax2.legend(); ax2.grid(True, linestyle='--', alpha=0.6); fig2.tight_layout()
    fig2.savefig(os.path.join(RESULTS_SAVE_PATH, f"loss_curve_{run_tag}.png"), dpi=300)
    print("      æŸå¤±æ›²çº¿å›¾å·²ä¿å­˜ã€‚")

    # 3. ç»¼åˆå››è”å›¾
    print("   -> æ­£åœ¨ç”Ÿæˆç»¼åˆæ€§èƒ½åˆ†æå›¾...")
    fig3, axes = plt.subplots(2, 2, figsize=(12, 10)); fig3.suptitle(f'çŸ¥ç§‹æ¨¡å‹ V{MODEL_VERSION} ç»¼åˆæ€§èƒ½è¯„ä¼°', fontsize=20)
    ax = axes[0, 0]; sample_indices = np.random.choice(len(y_true), size=min(50000, len(y_true)), replace=False)
    ax.scatter(y_true[sample_indices], y_pred_mean[sample_indices], alpha=0.1, s=5); ax.plot([0, 1], [0, 1], 'r--', lw=2)
    ax.set_title('åƒç´ çº§é¢„æµ‹ vs. çœŸå®å€¼ (æŠ½æ ·)', fontsize=14); ax.set_xlabel('çœŸå® SIC', fontsize=12); ax.set_ylabel('é¢„æµ‹ SIC', fontsize=12)
    ax.grid(True, linestyle='--', alpha=0.6); ax.text(0.05, 0.95, f"RÂ² = {results['r2']:.4f}\nRMSE = {results['rmse']:.4f}", transform=ax.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)); ax.set_aspect('equal', 'box')
    ax = axes[0, 1]; errors = (y_pred_mean - y_true).flatten()
    ax.hist(errors, bins=100, density=True, range=(-0.5, 0.5)); ax.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=14)
    ax.set_xlabel('é¢„æµ‹è¯¯å·® (é¢„æµ‹å€¼ - çœŸå®å€¼)', fontsize=12); ax.set_ylabel('æ¦‚ç‡å¯†åº¦', fontsize=12); ax.grid(True, linestyle='--', alpha=0.6); ax.axvline(0, color='red', linestyle='--')
    ax = axes[1, 0]; spatial_mae = np.mean(np.abs(y_pred_mean - y_true).reshape(-1, num_pixels), axis=0).reshape(grid_shape)
    im = ax.imshow(spatial_mae, cmap='Reds', origin='lower'); ax.set_title('ç©ºé—´å¹³å‡ç»å¯¹è¯¯å·® (MAE)', fontsize=14)
    ax.set_xticks([]); ax.set_yticks([]); fig3.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label='MAE')
    ax = axes[1, 1]; spatial_uncertainty = y_pred_std.reshape(-1, num_pixels).mean(axis=0).reshape(grid_shape)
    im = ax.imshow(spatial_uncertainty, cmap='viridis', origin='lower'); ax.set_title('ç©ºé—´å¹³å‡é¢„æµ‹ä¸ç¡®å®šæ€§ (Std Dev)', fontsize=14)
    ax.set_xticks([]); ax.set_yticks([]); fig3.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label='Std Dev')
    fig3.tight_layout(rect=[0, 0, 1, 0.96]); fig3.savefig(os.path.join(RESULTS_SAVE_PATH, f"comprehensive_analysis_{run_tag}.png"), dpi=300)
    print("      ç»¼åˆæ€§èƒ½å›¾å·²ä¿å­˜ã€‚")
    
    plt.close('all')
    print("ğŸ¨ æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæ¯•ã€‚")

# è°ƒç”¨å›¾è¡¨ç”Ÿæˆå‡½æ•°
generate_publication_plots(results, training_history, y_true, y_pred_mean, y_pred_std, sie_true_m, sie_pred_mean_m, test_time_coords_for_sie, grid_shape, RUN_TAG)

# ==============================================================================
# æ­¥éª¤ 12 (V8.5æ–°å¢): ä¿å­˜æ‰€æœ‰ç»˜å›¾æ‰€éœ€æ•°æ®
# ==============================================================================
print("\n--- æ­¥éª¤ 12: ä¿å­˜ç»˜å›¾æ•°æ® ---")

# ä¿å§†çº§æ³¨é‡Šï¼š
# æˆ‘ä»¬å°†æ‰€æœ‰ç”¨äºç”Ÿæˆä¸Šæ–¹å›¾è¡¨å’ŒæŠ¥å‘Šçš„å…³é”®å˜é‡ï¼Œæ‰“åŒ…åˆ°ä¸€ä¸ªPythonå­—å…¸(dictionary)ä¸­ã€‚
# è¿™ä¸ªå­—å…¸å°±åƒä¸€ä¸ªâ€œæ•°æ®å·¥å…·ç®±â€ï¼Œé‡Œé¢è£…æ»¡äº†æˆ‘ä»¬éœ€è¦çš„ä¸€åˆ‡ã€‚
# è¿™æ ·åšï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠè¿™ä¸ªâ€œå·¥å…·ç®±â€ä¸€æ¬¡æ€§ä¿å­˜èµ·æ¥ã€‚
plotting_data = {
    # 1. æ€»ç»“æ€§æŠ¥å‘Šå­—å…¸
    'results': results,
    # 2. å®Œæ•´çš„è®­ç»ƒå†å²è®°å½•
    'training_history': training_history,
    # 3. åºå¤§çš„åƒç´ çº§é¢„æµ‹æ•°ç»„
    'y_true': y_true,
    'y_pred_mean': y_pred_mean,
    'y_pred_std': y_pred_std,
    # 4. SIEæ—¶é—´åºåˆ—æ•°ç»„
    'sie_true_m': sie_true_m,
    'sie_pred_mean_m': sie_pred_mean_m,
    # 5. ç»˜å›¾æ‰€éœ€çš„é…ç½®å’Œå…ƒæ•°æ®
    'test_time_coords_for_sie': test_time_coords_for_sie,
    'grid_shape': grid_shape,
    'run_tag': RUN_TAG,
    'model_version': MODEL_VERSION
}

# å®šä¹‰ä¿å­˜è·¯å¾„ï¼Œæ–‡ä»¶ååŒ…å«æœ¬æ¬¡è¿è¡Œçš„å”¯ä¸€æ ‡è¯†(RUN_TAG)ï¼Œç¡®ä¿ä¸ä¼šè¦†ç›–æ—§ç»“æœã€‚
plotting_data_path = os.path.join(RESULTS_SAVE_PATH, f"plotting_data_{RUN_TAG}.pkl")

# ä½¿ç”¨Pythonçš„pickleåº“ï¼Œå°†æ•´ä¸ªâ€œæ•°æ®å·¥å…·ç®±â€å†™å…¥ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶(.pkl)ã€‚
# 'wb' ä¸­çš„ 'w' ä»£è¡¨å†™å…¥(write)ï¼Œ'b' ä»£è¡¨äºŒè¿›åˆ¶(binary)ã€‚
try:
    with open(plotting_data_path, 'wb') as f:
        pickle.dump(plotting_data, f)
    print(f"âœ… æ‰€æœ‰ç»˜å›¾æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {plotting_data_path}")
    print("   æ‚¨ç°åœ¨å¯ä»¥ç¼–å†™ä¸€ä¸ªç‹¬ç«‹çš„è„šæœ¬ï¼ŒåŠ è½½æ­¤æ–‡ä»¶æ¥å¿«é€Ÿé‡æ–°ç”Ÿæˆæˆ–è‡ªå®šä¹‰å›¾è¡¨ã€‚")
except Exception as e:
    print(f"âŒ ä¿å­˜ç»˜å›¾æ•°æ®å¤±è´¥: {e}")

print("\n" + "="*80)
print("ğŸ‰ çŸ¥ç§‹æ¨¡å‹ V8.5 - å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
print("="*80)